{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: CUDAdrv.jl failed to initialize, GPU functionality unavailable (set JULIA_CUDA_SILENT or JULIA_CUDA_VERBOSE to silence or expand this message)\n",
      "└ @ CUDAdrv /Users/hongbinren/.julia/packages/CUDAdrv/aBgcd/src/CUDAdrv.jl:69\n"
     ]
    }
   ],
   "source": [
    "using GPFlux\n",
    "using Flux\n",
    "using Zygote\n",
    "using Random; Random.seed!(5);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation of Gaussian process layer\n",
    "Traditional neural network layer is writen as:\n",
    "$$y=f(Wx+b)$$\n",
    "where $f$ is a nonlinear activation function, $W$ and $b$ are parameters. We can view this as a parametric function, another feature of this function is that it's deterministic.\n",
    "\n",
    "Since neural network layer is nothing but a parameterised nonlinear function, we are free to substitute it with Gaussian process. In Gaussian processes, the nonlinearity is obtained by directly model the distribution of function values $f$ as a multivariate Gaussian distribution, which is specified by it's mean and covariance function:\n",
    "$$\n",
    "\\begin{gather}\n",
    "f\\sim \\mathcal{GP}\\left(\\mu(x), K(x,x')\\right)\\\\\n",
    "y\\sim \\mathcal{N}(0,\\epsilon^2)\n",
    "\\end{gather}\n",
    "$$\n",
    "\n",
    "Compared to the normal neural network function, the Gaussian process approach has several advantages:\n",
    "* Non-parametric, it's realy parameter efficient ( the number of hyperparameters is significantly lower than the number of weights and biases in a common fully connected neural network ) and therefore isn't adapt to overfitting.\n",
    "* Enable us to deal with uncertainty\n",
    "* Allow us to explore the infinite-width limit of a neural network ( which by it's nature should be more expressive than finite width layer neural network )\n",
    "\n",
    "With a Gaussian process layer, it's feasible to implement several modern GP-related architecture:\n",
    "* GPLVM\n",
    "* Deep GP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example data\n",
    "X = rand(3, 5);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GaussProcess(ConstantMean([0.0]), IsoGaussKernel(ll=0.1, lσ=1.0), lnoise=-0.01)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# specify a Gaussian process with zero mean and isotropic square exponential kernel\n",
    "zero_mean = ConstantMean([0.0])\n",
    "iso_se_kernel = IsoGaussKernel([0.1], [1.0])  # length scale and overal scaling factor is in log scale\n",
    "lnoise = [-0.01]                              # ϵ is also in log scale\n",
    "gp = GaussProcess(zero_mean, iso_se_kernel, lnoise)    # build a Gaussian process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make Gaussian process a normal layer, we need a multi-output Gaussian process, this is down by assuming that each dimension of output is a independent draw from the Gaussian process prior ( this is a common approach taken in GPLVM and deep GP ):\n",
    "$$\n",
    "\\begin{gather}\n",
    "p(\\boldsymbol{y}|\\boldsymbol{X})=\\Pi_{i=1}^d\\,p(y_i|\\boldsymbol{X})\\\\\n",
    "y_i\\sim\\mathcal{GP}\\left(\\mu(\\boldsymbol{X}), K(\\boldsymbol{X, X})\\right)\n",
    "\\end{gather}\n",
    "$$\n",
    "where $d$ is the dimension of the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Params([[-0.005967428907752037 0.5589533448219299 -0.4503887891769409; 0.4754835069179535 0.5715048313140869 0.4152350127696991; … ; 0.37344714999198914 -0.10880734026432037 0.4539564251899719; -0.5453869104385376 -0.525185227394104 -0.028199106454849243], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0], [0.1], [1.0], [-0.01], [1.3460205793380737 0.7891872525215149], [0.0]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Implementation of a GP layer:\n",
    "    GP layer is specified by a Gaussian process and the dimension of the output\n",
    "\n",
    "here I write a simplified version of MvNormal, which is compatible with Distribution.jl, and use reparametrization\n",
    "trick ( Kingma, Welling., Auto-Encoding Variational Bayes, arXiv:1312.6114 ) to allow backpropogation through sampling process.\n",
    "\"\"\"\n",
    "\n",
    "struct GPLayer{GPT<:GaussProcess, T<:Int}\n",
    "    gp::GPT\n",
    "    odims::T\n",
    "end\n",
    "Flux.@functor GPLayer\n",
    "function (gpl::GPLayer)(x)\n",
    "    xo = rand(MvNormal(gpl.gp, x), gpl.odims) # MvNormal{<:GaussProcess, <:AbstractArray} <: ContinuousMultivariateDistribution\n",
    "    transpose(xo)\n",
    "end\n",
    "\n",
    "model = Chain(Dense(3, 10), GPLayer(gp, 2), Dense(2, 1)) |> f64\n",
    "ps = params(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1×5 Array{Float64,2}:\n",
       " 6.06098  -1.07062  4.78621  4.83658  -1.30697"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# forawrd pass\n",
    "model(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "IdDict{Any,Any} with 8 entries:\n",
       "  [1.0]                     => [12.8382]\n",
       "  [0.0]                     => [10.676]\n",
       "  [-0.00596743 0.558953 -0… => [0.484082 -1.28918 1.26373; 1.01697 0.0246278 -0…\n",
       "  [0.1]                     => [6.98602]\n",
       "  [-0.01]                   => [3.71533]\n",
       "  [0.0, 0.0, 0.0, 0.0, 0.0… => [-2.22045e-16, 2.22045e-16, 8.88178e-16, -8.8817…\n",
       "  [1.34602 0.789187]        => [7.65121 7.92573]\n",
       "  [0.0]                     => [5.0]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# evaluate the derivative w.r.t to all the parameters\n",
    "gs = gradient(()->sum(model(X)), ps)\n",
    "gs.grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Params([[0.0], [4.0], [1.0], [-0.1], [0.0], [0.0], [0.0], [1.0], [-0.1]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we can also write deep GP\n",
    "zero_mean1 = ConstantMean()\n",
    "se_kernel1 = IsoGaussKernel([4.0], [1.0])\n",
    "lnoise1 = [-0.1]\n",
    "gp1 = GaussProcess(zero_mean1, se_kernel1, lnoise1)\n",
    "\n",
    "zero_mean2 = ConstantMean()\n",
    "per_kernel2 = IsoPeriodKernel([0.0], [0.0], [1.0])\n",
    "lnoise2 = [-0.1]\n",
    "gp2 = GaussProcess(zero_mean2, per_kernel2, lnoise2)\n",
    "\n",
    "DeepGP = Chain(GPLayer(gp1, 2), GPLayer(gp2, 5))\n",
    "ps = params(DeepGP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5×5 LinearAlgebra.Transpose{Float64,Array{Float64,2}}:\n",
       "  2.0988     -1.93274    4.03467   -4.8294     3.17422\n",
       "  0.0968363   3.13354    0.271508  -0.630904   1.54851\n",
       " -0.247104    1.38856   -2.38596    3.72538   -1.04662\n",
       " -3.88221    -1.61955    2.77025    1.52945   -2.82033\n",
       "  1.51003     0.921324  -1.70876   -3.51345   -1.36152"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# forward pass\n",
    "\"\"\"\n",
    "Warning: may throw positive definite error when evaluating cholesky factorization, this will be studied\n",
    "carefully later\n",
    "\"\"\"\n",
    "DeepGP(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "IdDict{Any,Any} with 9 entries:\n",
       "  [0.0]  => [-5.32907e-15]\n",
       "  [1.0]  => [-2.47095]\n",
       "  [4.0]  => [-0.0433978]\n",
       "  [-0.1] => [108.323]\n",
       "  [0.0]  => [25.0]\n",
       "  [0.0]  => [-105.852]\n",
       "  [1.0]  => [-1.64464]\n",
       "  [-0.1] => [2.51644]\n",
       "  [0.0]  => [-20.7026]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# evaluate the derivative w.r.t to all the parameters\n",
    "gs = gradient(()->sum(DeepGP(X)), ps)\n",
    "gs.grads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Something we could discuss\n",
    "* Implement training algorithm based on variational inference\n",
    "* How to validate the gradient for a stochastic function ( I'm wondering if finite difference method is still a good validate method as it is for deterministic function, currently I find there is a large discrepency between the derivative evaluated by backprop and finite difference for stochastic function, maybe I'm wrong, but we can figure this out later )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "[1] Andreas C. Damianou, Neil D. Lawrence, Deep Gaussian Processes, 2013\n",
    "\n",
    "[2] Diederik P. Kingma, Max Welling, Auto-Encoding Variational Bayes, 2014\n",
    "\n",
    "[3] Michalis K. Titsias1, Neil D. Lawrence, Bayesian Gaussian Process Latent Variable Model, 2010\n",
    "\n",
    "[4] Danilo J. Rezende, Shakir Mohamed, Daan Wierstra, Stochastic Backpropogation and Approximate Inference in Deep Generative Models, 2014"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "julia 1.3.0",
   "language": "julia",
   "name": "julia-1.3"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.3.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
